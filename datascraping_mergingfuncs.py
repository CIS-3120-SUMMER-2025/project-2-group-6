"""Project Q1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1INfWBaUrxsNuHc4M3c9n3LpcVjtMbGFM
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import unicodedata
import os 
import gradio as gr
from dotenv import load_dotenv 
import sqlite3

url = 'https://www.worldometers.info/world-population/population-by-country/'

page = requests.get(url)

page.status_code

soup = BeautifulSoup(page.content, 'html.parser')

rows = soup.find_all('tr')[1:]
print(len(rows))

country =[]

for row in rows:
  td_tags = row.find('a', class_="transition-all duration-200 text-lime-600 hover:text-lime-500").get_text()
  country.append(td_tags)

print(country)

rows[0].find_all('td')[2].get_text()

population = []
year_change =[]
net_change =[]
dencity =[]
land_area =[]
migrants =[]
fert_rate=[]
median_age=[]
urban_pop=[]
world_share=[]

for row in rows:
  td_tag = row.find_all('td')

  if len(td_tag)==12:
    population.append(td_tag[2].get_text())
  year_change.append(td_tag[3].get_text())
  net_change.append(td_tag[4].get_text())
  dencity.append(td_tag[5].get_text())
  land_area.append(td_tag[6].get_text())
  migrants.append(td_tag[7].get_text())
  fert_rate.append(td_tag[8].get_text())
  median_age.append(td_tag[9].get_text())
  urban_pop.append(td_tag[10].get_text())
  world_share.append(td_tag[11].get_text())

print(population)
print(year_change)
print(net_change)
print(dencity)
print(land_area)
print(migrants)
print(fert_rate)
print(median_age)
print(urban_pop)
print(world_share)

print(len(world_share))

saving_data = {
              'Country': country,
              'Population 2025': population,
              'Yearly Change': year_change,
              'Net Change': net_change,
              'Density(P/km^2)': dencity,
              'Land Area(Km^2)': land_area,
              'Migrants(net)' : migrants,
              'Fert. Rate': fert_rate,
              'Median Age': median_age,
              'Urban Pop%': urban_pop,
              'World Share': world_share
}

df1 = pd.DataFrame(saving_data)

df1.head()

df1.describe()

df1.info()


base_url = "https://api.weatherapi.com/v1/forecast.json"
load_dotenv()

WEATHER_API_KEY = os.getenv("WEATHER_API_KEY")
if not WEATHER_API_KEY:
    print("Warning: WEATHER_API_KEY not found in environment variables.")
    print("Please create a .env file with your WEATHER_API_KEY.")



def get_weatherdata(df1):
    rows = []
    for country in df1["Country"]:  
        country = str(country).strip()

        r = requests.get(
            base_url, 
            params={"key": WEATHER_API_KEY, "q": country, "days": 1, "aqi": "no", "alerts": "no"},
            timeout = 20
        )
        data = r.json()
        forecast_data = data.get("forecast", {}).get("forecastday", [])

        if forecast_data:
            day = forecast_data[0].get("day", {})
        else:
            day = {}

        rows.append({
            "Country": country,  
            "maxtemp_f": day.get("maxtemp_f"),
            "mintemp_f": day.get("mintemp_f"),
            "totalprecip_in": day.get("totalprecip_in"),
            "avghumidity": day.get("avghumidity"),
            "uv": day.get("uv")
        })
    return pd.DataFrame(rows)

df2 = get_weatherdata(df1)
df2 = get_weatherdata(df1)
os.makedirs('data', exist_ok=True)
df2.to_csv("data/df2.csv", index=False)

print(df2)
df3 = df1.merge(df2, on="Country", how="left")

print(df3.describe())

df3.to_csv("df3_combined.csv", index=False)

with sqlite3.connect("df3_combined.db") as conn:
    df3.to_sql("df3", conn, if_exists="replace", index=False)
